layout: page
title: "RELATED WORK"
permalink: https://alexisjcarras.github.io/Subjectivity-Classifier-/relatedwork

# Related work

Past research in subjectivity analysis has employed rule-based, lexicon-based (i.e. containing cues often found in subjective text), unsupervised, and supervised approaches (Wiebe et al., 2004). More recent approaches by Pryzant, Martinez, Das, Kurohashi, Jurafsky, and Yang (2019) have used a BERT model that finds specific words in a text that reveal a private state and replaces them with neutral words. A lot of research has been made on the identification of cues that make a sentence subjective. For example, Recasens et al., (2013, p. 1650) developed a set of linguistic features “including factive verbs, implicative, hedges, and subjective intensifiers” which are often found in subjective text. In general, past work in the field has focused on document-level bias, or word-level bias. That is, classifying a whole text as biased and identifying the bias inducing word in a sentence, that is the word revealing some private, subjective view. Other notable work by Pang and Lee (2004) uses graph theory and Naïve Bayes classifier to identify bias in a series of related sentences.


This project was greatly insipired by the paper titled: ["Automatically Neutralizing Subjective Bias in Text"](https://arxiv.org/abs/1911.09709)
